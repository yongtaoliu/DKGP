{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Kernel GP Classification Tutorial\n",
    "\n",
    "This notebook demonstrates how to use Deep Kernel GP for classification tasks:\n",
    "1. Binary Classification\n",
    "2. Multi-Class Classification\n",
    "3. Uncertainty Quantification\n",
    "4. Decision Boundaries\n",
    "5. Model Evaluation\n",
    "\n",
    "**Author:** Deep Kernel GP Team  \n",
    "**Date:** February 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup & Installation\n",
    "\n",
    "Make sure the package is installed:\n",
    "\n",
    "```bash\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    confusion_matrix, \n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Deep Kernel GP Classification\n",
    "from deep_kernel_gp.classification import (\n",
    "    DeepKernelGPClassifier,\n",
    "    fit_dkgp_classifier,\n",
    "    predict_classifier\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Deep Kernel GP Classification imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1Ô∏è‚É£ Binary Classification: Moons Dataset\n",
    "\n",
    "Let's start with a classic non-linearly separable dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate moons dataset (2D)\n",
    "X_2d, y_binary = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "\n",
    "# Visualize 2D data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_2d[y_binary==0, 0], X_2d[y_binary==0, 1], \n",
    "           c='#2E86AB', s=50, alpha=0.7, edgecolors='k', label='Class 0')\n",
    "plt.scatter(X_2d[y_binary==1, 0], X_2d[y_binary==1, 1], \n",
    "           c='#A23B72', s=50, alpha=0.7, edgecolors='k', label='Class 1')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Moons Dataset (2D)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Dataset shape: {X_2d.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_binary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add extra dimensions to make it high-dimensional\n",
    "# This simulates real-world scenarios with many features\n",
    "n_extra_dims = 48\n",
    "X_binary = np.hstack([X_2d, np.random.randn(300, n_extra_dims)])\n",
    "\n",
    "print(f\"High-dimensional dataset shape: {X_binary.shape}\")\n",
    "print(f\"Total features: {X_binary.shape[1]} (2 informative + {n_extra_dims} noise)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(\n",
    "    X_binary, y_binary, test_size=0.3, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_bin.shape}\")\n",
    "print(f\"Test set: {X_test_bin.shape}\")\n",
    "print(f\"Training class distribution: {np.bincount(y_train_bin)}\")\n",
    "print(f\"Test class distribution: {np.bincount(y_test_bin)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "model_binary, losses_binary = fit_dkgp_classifier(\n",
    "    X_train_bin,\n",
    "    y_train_bin,\n",
    "    num_classes=2,           # Binary classification\n",
    "    feature_dim=16,          # Learned feature dimension\n",
    "    hidden_dims=[128, 64],   # Neural network architecture\n",
    "    num_inducing=50,         # Number of inducing points\n",
    "    num_epochs=800,          # Training iterations\n",
    "    lr_features=1e-4,        # Learning rate for features\n",
    "    lr_gp=1e-2,             # Learning rate for GP\n",
    "    verbose=True,\n",
    "    plot_loss=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses_binary, linewidth=2, color='#2E86AB')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Negative ELBO', fontsize=12)\n",
    "plt.title('Training Loss', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(losses_binary[100:], linewidth=2, color='#A23B72')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Negative ELBO', fontsize=12)\n",
    "plt.title('Training Loss (after epoch 100)', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {losses_binary[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class labels\n",
    "y_pred_bin = predict_classifier(model_binary, X_test_bin)\n",
    "\n",
    "# Predict class probabilities\n",
    "y_proba_bin = predict_classifier(model_binary, X_test_bin, return_proba=True)\n",
    "\n",
    "print(f\"Predictions shape: {y_pred_bin.shape}\")\n",
    "print(f\"Probabilities shape: {y_proba_bin.shape}\")\n",
    "print(f\"\\nFirst 5 predictions:\")\n",
    "print(f\"  True:  {y_test_bin[:5]}\")\n",
    "print(f\"  Pred:  {y_pred_bin[:5]}\")\n",
    "print(f\"  Prob Class 0: {y_proba_bin[:5, 0]}\")\n",
    "print(f\"  Prob Class 1: {y_proba_bin[:5, 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_test_bin, y_pred_bin)\n",
    "precision = precision_score(y_test_bin, y_pred_bin)\n",
    "recall = recall_score(y_test_bin, y_pred_bin)\n",
    "f1 = f1_score(y_test_bin, y_pred_bin)\n",
    "\n",
    "# Average confidence\n",
    "confidence = y_proba_bin.max(axis=1).mean()\n",
    "\n",
    "print(\"Binary Classification Results\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"Avg Confidence: {confidence:.4f}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test_bin, y_pred_bin, target_names=['Class 0', 'Class 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# 1. Decision boundary (2D projection)\n",
    "x_min, x_max = X_2d[:, 0].min() - 0.5, X_2d[:, 0].max() + 0.5\n",
    "y_min, y_max = X_2d[:, 1].min() - 0.5, X_2d[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "mesh_2d = np.c_[xx.ravel(), yy.ravel()]\n",
    "mesh_high = np.hstack([mesh_2d, np.zeros((len(mesh_2d), n_extra_dims))])\n",
    "Z = predict_classifier(model_binary, mesh_high)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "axes[0, 0].contourf(xx, yy, Z, alpha=0.3, levels=1, colors=['#2E86AB', '#A23B72'])\n",
    "scatter = axes[0, 0].scatter(X_test_bin[:, 0], X_test_bin[:, 1], \n",
    "                            c=y_test_bin, s=80, edgecolors='k', \n",
    "                            linewidth=1.5, cmap='coolwarm', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Feature 1', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Feature 2', fontsize=11)\n",
    "axes[0, 0].set_title(f'Decision Boundary (Acc={accuracy:.3f})', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=axes[0, 0])\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "cm = confusion_matrix(y_test_bin, y_pred_bin)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1],\n",
    "           cbar=False, square=True, annot_kws={'size': 14})\n",
    "axes[0, 1].set_xlabel('Predicted', fontsize=11)\n",
    "axes[0, 1].set_ylabel('True', fontsize=11)\n",
    "axes[0, 1].set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 3. ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test_bin, y_proba_bin[:, 1])\n",
    "auc = roc_auc_score(y_test_bin, y_proba_bin[:, 1])\n",
    "axes[0, 2].plot(fpr, tpr, linewidth=3, label=f'AUC = {auc:.3f}', color='#2E86AB')\n",
    "axes[0, 2].plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.5)\n",
    "axes[0, 2].set_xlabel('False Positive Rate', fontsize=11)\n",
    "axes[0, 2].set_ylabel('True Positive Rate', fontsize=11)\n",
    "axes[0, 2].set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
    "axes[0, 2].legend(fontsize=10)\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Prediction Confidence\n",
    "confidence_scores = y_proba_bin.max(axis=1)\n",
    "correct = y_test_bin == y_pred_bin\n",
    "colors = ['#2E86AB' if c else '#C73E1D' for c in correct]\n",
    "axes[1, 0].scatter(range(len(confidence_scores)), confidence_scores,\n",
    "                  c=colors, s=60, alpha=0.6, edgecolors='k', linewidth=0.5)\n",
    "axes[1, 0].axhline(y=0.5, color='r', linestyle='--', linewidth=2, alpha=0.5)\n",
    "axes[1, 0].set_xlabel('Test Sample', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Prediction Confidence', fontsize=11)\n",
    "axes[1, 0].set_title('Confidence (Blue=Correct, Red=Wrong)', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 5. Confidence Distribution\n",
    "axes[1, 1].hist(confidence_scores[correct], bins=15, alpha=0.7, \n",
    "               label='Correct', color='#2E86AB', edgecolor='k')\n",
    "axes[1, 1].hist(confidence_scores[~correct], bins=15, alpha=0.7, \n",
    "               label='Wrong', color='#C73E1D', edgecolor='k')\n",
    "axes[1, 1].set_xlabel('Confidence', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 1].set_title('Confidence Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 6. Class Probabilities\n",
    "for i in range(2):\n",
    "    class_probs = y_proba_bin[y_test_bin == i, i]\n",
    "    axes[1, 2].hist(class_probs, bins=15, alpha=0.6, \n",
    "                   label=f'Class {i}', edgecolor='k')\n",
    "axes[1, 2].set_xlabel('Predicted Probability', fontsize=11)\n",
    "axes[1, 2].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 2].set_title('Probability Distribution by True Class', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "axes[1, 2].legend(fontsize=10)\n",
    "axes[1, 2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze uncertainty\n",
    "uncertainty = 1 - confidence_scores  # Uncertainty = 1 - max probability\n",
    "\n",
    "# Find most confident and uncertain predictions\n",
    "most_confident_idx = np.argsort(confidence_scores)[-5:][::-1]\n",
    "most_uncertain_idx = np.argsort(confidence_scores)[:5]\n",
    "\n",
    "print(\"Most Confident Predictions:\")\n",
    "print(\"=\" * 50)\n",
    "for idx in most_confident_idx:\n",
    "    print(f\"Sample {idx}: True={y_test_bin[idx]}, Pred={y_pred_bin[idx]}, \"\n",
    "          f\"Confidence={confidence_scores[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nMost Uncertain Predictions:\")\n",
    "print(\"=\" * 50)\n",
    "for idx in most_uncertain_idx:\n",
    "    print(f\"Sample {idx}: True={y_test_bin[idx]}, Pred={y_pred_bin[idx]}, \"\n",
    "          f\"Confidence={confidence_scores[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2Ô∏è‚É£ Multi-Class Classification\n",
    "\n",
    "Now let's tackle a 4-class classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Multi-Class Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic multi-class data\n",
    "X_multi, y_multi = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=20,\n",
    "    n_informative=12,\n",
    "    n_redundant=5,\n",
    "    n_classes=4,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Add more dimensions\n",
    "X_multi = np.hstack([X_multi, np.random.randn(500, 30)])\n",
    "\n",
    "print(f\"Multi-class dataset shape: {X_multi.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_multi))}\")\n",
    "print(f\"Class distribution: {np.bincount(y_multi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 2 dimensions\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_multi[:, 0], X_multi[:, 1], c=y_multi, \n",
    "                     s=50, alpha=0.7, edgecolors='k', cmap='tab10')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Multi-Class Dataset (2D Projection)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, label='Class')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_multi, y_multi, test_size=0.3, random_state=42, stratify=y_multi\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_m.shape}\")\n",
    "print(f\"Test set: {X_test_m.shape}\")\n",
    "print(f\"Training class distribution: {np.bincount(y_train_m)}\")\n",
    "print(f\"Test class distribution: {np.bincount(y_test_m)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Multi-Class Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multi-class classifier\n",
    "model_multi, losses_multi = fit_dkgp_classifier(\n",
    "    X_train_m,\n",
    "    y_train_m,\n",
    "    num_classes=4,           # 4 classes\n",
    "    feature_dim=16,\n",
    "    hidden_dims=[256, 128, 64],\n",
    "    num_inducing=100,        # More inducing points for multi-class\n",
    "    num_epochs=800,\n",
    "    lr_features=1e-4,\n",
    "    lr_gp=1e-2,\n",
    "    verbose=True,\n",
    "    plot_loss=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses_multi, linewidth=2, color='#F18F01')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Negative ELBO', fontsize=12)\n",
    "plt.title('Multi-Class Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {losses_multi[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Multi-Class Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_m = predict_classifier(model_multi, X_test_m)\n",
    "y_proba_m = predict_classifier(model_multi, X_test_m, return_proba=True)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy_m = accuracy_score(y_test_m, y_pred_m)\n",
    "precision_m = precision_score(y_test_m, y_pred_m, average='weighted')\n",
    "recall_m = recall_score(y_test_m, y_pred_m, average='weighted')\n",
    "f1_m = f1_score(y_test_m, y_pred_m, average='weighted')\n",
    "\n",
    "print(\"Multi-Class Classification Results\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Accuracy:  {accuracy_m:.4f}\")\n",
    "print(f\"Precision: {precision_m:.4f}\")\n",
    "print(f\"Recall:    {recall_m:.4f}\")\n",
    "print(f\"F1 Score:  {f1_m:.4f}\")\n",
    "print(f\"Avg Confidence: {y_proba_m.max(axis=1).mean():.4f}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test_m, y_pred_m, \n",
    "                          target_names=[f'Class {i}' for i in range(4)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Multi-Class Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "cm_m = confusion_matrix(y_test_m, y_pred_m)\n",
    "sns.heatmap(cm_m, annot=True, fmt='d', cmap='Oranges', ax=axes[0, 0],\n",
    "           square=True, cbar_kws={'label': 'Count'})\n",
    "axes[0, 0].set_xlabel('Predicted', fontsize=11)\n",
    "axes[0, 0].set_ylabel('True', fontsize=11)\n",
    "axes[0, 0].set_title(f'Confusion Matrix (Acc={accuracy_m:.3f})', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "\n",
    "# 2. Per-Class Accuracy\n",
    "per_class_acc = []\n",
    "for i in range(4):\n",
    "    mask = y_test_m == i\n",
    "    acc_i = (y_test_m[mask] == y_pred_m[mask]).mean() if mask.sum() > 0 else 0\n",
    "    per_class_acc.append(acc_i)\n",
    "\n",
    "bars = axes[0, 1].bar(range(4), per_class_acc, color='#A23B72', \n",
    "                     alpha=0.8, edgecolor='k', linewidth=1.5)\n",
    "axes[0, 1].axhline(y=accuracy_m, color='r', linestyle='--', \n",
    "                  linewidth=2, label=f'Overall: {accuracy_m:.3f}')\n",
    "axes[0, 1].set_xlabel('Class', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0, 1].set_title('Per-Class Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylim([0, 1.1])\n",
    "axes[0, 1].set_xticks(range(4))\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add values on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{per_class_acc[i]:.3f}',\n",
    "                   ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 3. Probability Distribution by True Class\n",
    "for i in range(4):\n",
    "    class_probs = y_proba_m[y_test_m == i, i]\n",
    "    axes[0, 2].hist(class_probs, bins=12, alpha=0.6, \n",
    "                   label=f'Class {i}', edgecolor='k')\n",
    "axes[0, 2].set_xlabel('Predicted Probability', fontsize=11)\n",
    "axes[0, 2].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0, 2].set_title('Probability Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 2].legend(fontsize=9)\n",
    "axes[0, 2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Confidence by Correctness\n",
    "confidence_m = y_proba_m.max(axis=1)\n",
    "correct_m = y_test_m == y_pred_m\n",
    "axes[1, 0].hist(confidence_m[correct_m], bins=15, alpha=0.7, \n",
    "               label='Correct', color='#2E86AB', edgecolor='k')\n",
    "axes[1, 0].hist(confidence_m[~correct_m], bins=15, alpha=0.7, \n",
    "               label='Wrong', color='#C73E1D', edgecolor='k')\n",
    "axes[1, 0].set_xlabel('Prediction Confidence', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 0].set_title('Confidence Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 5. Per-Class Precision, Recall, F1\n",
    "metrics_per_class = []\n",
    "for i in range(4):\n",
    "    precision_i = precision_score(y_test_m == i, y_pred_m == i, zero_division=0)\n",
    "    recall_i = recall_score(y_test_m == i, y_pred_m == i, zero_division=0)\n",
    "    f1_i = f1_score(y_test_m == i, y_pred_m == i, zero_division=0)\n",
    "    metrics_per_class.append([precision_i, recall_i, f1_i])\n",
    "\n",
    "metrics_per_class = np.array(metrics_per_class)\n",
    "x = np.arange(4)\n",
    "width = 0.25\n",
    "axes[1, 1].bar(x - width, metrics_per_class[:, 0], width, \n",
    "              label='Precision', alpha=0.8, edgecolor='k')\n",
    "axes[1, 1].bar(x, metrics_per_class[:, 1], width, \n",
    "              label='Recall', alpha=0.8, edgecolor='k')\n",
    "axes[1, 1].bar(x + width, metrics_per_class[:, 2], width, \n",
    "              label='F1', alpha=0.8, edgecolor='k')\n",
    "axes[1, 1].set_xlabel('Class', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Score', fontsize=11)\n",
    "axes[1, 1].set_title('Per-Class Metrics', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_ylim([0, 1.1])\n",
    "axes[1, 1].legend(fontsize=9)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 6. Prediction Examples\n",
    "# Show some correct and incorrect predictions\n",
    "correct_idx = np.where(correct_m)[0][:5]\n",
    "wrong_idx = np.where(~correct_m)[0][:5]\n",
    "\n",
    "example_text = \"Correct Predictions:\\n\"\n",
    "for idx in correct_idx:\n",
    "    example_text += f\"True: {y_test_m[idx]}, Pred: {y_pred_m[idx]}, \"\n",
    "    example_text += f\"Conf: {confidence_m[idx]:.3f}\\n\"\n",
    "\n",
    "example_text += \"\\nIncorrect Predictions:\\n\"\n",
    "for idx in wrong_idx:\n",
    "    example_text += f\"True: {y_test_m[idx]}, Pred: {y_pred_m[idx]}, \"\n",
    "    example_text += f\"Conf: {confidence_m[idx]:.3f}\\n\"\n",
    "\n",
    "axes[1, 2].text(0.05, 0.95, example_text, transform=axes[1, 2].transAxes,\n",
    "               fontsize=10, verticalalignment='top', family='monospace',\n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "axes[1, 2].set_title('Example Predictions', fontsize=12, fontweight='bold')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3Ô∏è‚É£ Comparison: Binary vs Multi-Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Avg Confidence'],\n",
    "    'Binary': [accuracy, precision, recall, f1, confidence],\n",
    "    'Multi-Class': [accuracy_m, precision_m, recall_m, f1_m, y_proba_m.max(axis=1).mean()]\n",
    "})\n",
    "\n",
    "print(\"\\nPerformance Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Metrics comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "binary_scores = [accuracy, precision, recall, f1]\n",
    "multi_scores = [accuracy_m, precision_m, recall_m, f1_m]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "axes[0].bar(x - width/2, binary_scores, width, label='Binary', alpha=0.8)\n",
    "axes[0].bar(x + width/2, multi_scores, width, label='Multi-Class', alpha=0.8)\n",
    "axes[0].set_xlabel('Metric', fontsize=12)\n",
    "axes[0].set_ylabel('Score', fontsize=12)\n",
    "axes[0].set_title('Performance Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics)\n",
    "axes[0].set_ylim([0, 1.1])\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Training loss comparison\n",
    "axes[1].plot(losses_binary, label='Binary', linewidth=2, alpha=0.8)\n",
    "axes[1].plot(losses_multi, label='Multi-Class', linewidth=2, alpha=0.8)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Negative ELBO', fontsize=12)\n",
    "axes[1].set_title('Training Loss Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. ‚úÖ **Binary Classification**: Moons dataset with non-linear boundary\n",
    "2. ‚úÖ **Multi-Class Classification**: 4-class synthetic dataset\n",
    "3. ‚úÖ **Uncertainty Quantification**: Confidence scores and distributions\n",
    "4. ‚úÖ **Decision Boundaries**: Visualization of learned boundaries\n",
    "5. ‚úÖ **Comprehensive Evaluation**: Metrics, confusion matrices, ROC curves\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Deep Kernel GP** handles high-dimensional classification well\n",
    "- **Uncertainty estimates** are well-calibrated\n",
    "- **Multi-class** works as easily as binary\n",
    "- **Variational inference** makes it scalable\n",
    "- **Probabilistic predictions** enable better decision-making\n",
    "\n",
    "### When to Use GP Classification\n",
    "\n",
    "‚úÖ When you need **uncertainty quantification**  \n",
    "‚úÖ When you have **high-dimensional** inputs  \n",
    "‚úÖ When you want **probabilistic** predictions  \n",
    "‚úÖ When your data is **relatively small** (100s-1000s)  \n",
    "‚úÖ When interpretability matters\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try on your own classification dataset\n",
    "- Experiment with different architectures\n",
    "- Tune `num_inducing` for speed/accuracy trade-off\n",
    "- Combine with active learning strategies\n",
    "- Use uncertainty for outlier detection\n",
    "\n",
    "Happy classifying! üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
