{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DKGPR\n",
    "\n",
    "This notebook demonstrates how to use the DKGPR for:\n",
    "1. Basic regression\n",
    "2. Uncertainty quantification\n",
    "3. Confidence weighting\n",
    "4. Bayesian optimization\n",
    "\n",
    "**Author:** Yongtao Liu \n",
    "\n",
    "**Date:** January 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, make sure the package is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.4.2\n",
      "PyTorch version: 2.10.0\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DKGP\n",
    "from deep_kernel_gp import fit_dkgp, predict\n",
    "from deep_kernel_gp.acquisition import (\n",
    "    expected_improvement,\n",
    "    upper_confidence_bound,\n",
    "    probability_of_improvement\n",
    ")\n",
    "from deep_kernel_gp.prediction import predict_with_epistemic_aleatoric\n",
    "from deep_kernel_gp.utils import (\n",
    "    standardize_data,\n",
    "    compute_metrics,\n",
    "    compute_calibration,\n",
    "    print_metrics\n",
    ")\n",
    "\n",
    "print(\"âœ… Deep Kernel GP imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1ï¸âƒ£ Basic Regression\n",
    "\n",
    "Let's start with a simple regression problem on high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples=100, input_dim=50, noise=0.1):\n",
    "    \"\"\"\n",
    "    Generate synthetic high-dimensional data.\n",
    "    True function: non-linear combination of first few features\n",
    "    \"\"\"\n",
    "    X = np.random.randn(n_samples, input_dim)\n",
    "    \n",
    "    # True underlying function (only first 5 features matter)\n",
    "    y = (X[:, 0] + \n",
    "         2 * X[:, 1] - \n",
    "         X[:, 2] + \n",
    "         0.5 * X[:, 3]**2 + \n",
    "         np.sin(X[:, 4]) + \n",
    "         noise * np.random.randn(n_samples))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate training data\n",
    "X_train, y_train = generate_data(n_samples=200, input_dim=100, noise=0.1)\n",
    "\n",
    "# Generate test data\n",
    "X_test, y_test = generate_data(n_samples=50, input_dim=100, noise=0.1)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Target range: [{y_train.min():.2f}, {y_train.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Deep Kernel GP\n",
    "mll, gp_model, dkl_model, losses = fit_dkgp(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    feature_dim=16,           # Reduce 100D -> 16D\n",
    "    hidden_dims=[256, 128, 64],  # Neural network architecture\n",
    "    num_epochs=1000,          # Training iterations\n",
    "    lr_features=1e-4,         # Learning rate for neural network\n",
    "    lr_gp=1e-2,              # Learning rate for GP\n",
    "    verbose=True,\n",
    "    plot_loss=False          # We'll plot manually\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, linewidth=2, color='#2E86AB')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Negative Log Likelihood', fontsize=12)\n",
    "plt.title('Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(losses[100:], linewidth=2, color='#A23B72')  # Skip first 100 epochs\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Negative Log Likelihood', fontsize=12)\n",
    "plt.title('Training Loss (after epoch 100)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "mean_pred, std_pred = predict(dkl_model, X_test, return_std=True)\n",
    "\n",
    "print(f\"Predictions shape: {mean_pred.shape}\")\n",
    "print(f\"Uncertainty shape: {std_pred.shape}\")\n",
    "print(f\"Mean uncertainty: {std_pred.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "metrics = compute_metrics(y_test, mean_pred, std_pred)\n",
    "print_metrics(metrics, \"Test Set Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. Predictions vs True\n",
    "axes[0].scatter(y_test, mean_pred, alpha=0.6, s=80, edgecolors='k', linewidth=1)\n",
    "min_val = min(y_test.min(), mean_pred.min())\n",
    "max_val = max(y_test.max(), mean_pred.max())\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect')\n",
    "axes[0].set_xlabel('True Values', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Values', fontsize=12)\n",
    "axes[0].set_title(f'Predictions (RÂ²={metrics[\"r2\"]:.3f})', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Residuals\n",
    "residuals = y_test - mean_pred\n",
    "axes[1].scatter(mean_pred, residuals, alpha=0.6, s=80, edgecolors='k', linewidth=1)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Values', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals', fontsize=12)\n",
    "axes[1].set_title('Residual Plot', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Predictions with uncertainty\n",
    "sorted_idx = np.argsort(y_test)\n",
    "x_axis = np.arange(len(y_test))\n",
    "\n",
    "axes[2].plot(x_axis, y_test[sorted_idx], 'o-', label='True', linewidth=2, markersize=7)\n",
    "axes[2].plot(x_axis, mean_pred[sorted_idx], 's-', label='Predicted', linewidth=2, markersize=7)\n",
    "axes[2].fill_between(\n",
    "    x_axis,\n",
    "    mean_pred[sorted_idx] - 2*std_pred[sorted_idx],\n",
    "    mean_pred[sorted_idx] + 2*std_pred[sorted_idx],\n",
    "    alpha=0.3,\n",
    "    label='95% CI'\n",
    ")\n",
    "axes[2].set_xlabel('Test Sample (sorted)', fontsize=12)\n",
    "axes[2].set_ylabel('Value', fontsize=12)\n",
    "axes[2].set_title('Predictions with Uncertainty', fontsize=13, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2ï¸âƒ£ Uncertainty Quantification\n",
    "\n",
    "Deep Kernel GP provides two types of uncertainty:\n",
    "- **Epistemic**: Model uncertainty (reducible with more data)\n",
    "- **Aleatoric**: Observation noise (irreducible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose uncertainty\n",
    "mean_unc, epistemic, aleatoric, total = predict_with_epistemic_aleatoric(\n",
    "    dkl_model, X_test\n",
    ")\n",
    "\n",
    "# Convert to std (from variance)\n",
    "epistemic_std = np.sqrt(epistemic)\n",
    "aleatoric_std = np.sqrt(aleatoric)\n",
    "total_std = np.sqrt(total)\n",
    "\n",
    "print(\"Uncertainty Decomposition:\")\n",
    "print(f\"  Epistemic (model):      {epistemic_std.mean():.4f} Â± {epistemic_std.std():.4f}\")\n",
    "print(f\"  Aleatoric (noise):      {aleatoric_std.mean():.4f} Â± {aleatoric_std.std():.4f}\")\n",
    "print(f\"  Total:                  {total_std.mean():.4f} Â± {total_std.std():.4f}\")\n",
    "print(f\"  Epistemic/Aleatoric:    {epistemic_std.mean()/aleatoric_std.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize uncertainty decomposition\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Stacked bar chart\n",
    "x_axis = np.arange(len(y_test))\n",
    "axes[0].bar(x_axis, epistemic_std, label='Epistemic', alpha=0.8, color='#F18F01')\n",
    "axes[0].bar(x_axis, aleatoric_std, bottom=epistemic_std, label='Aleatoric', alpha=0.8, color='#C73E1D')\n",
    "axes[0].set_xlabel('Test Sample', fontsize=12)\n",
    "axes[0].set_ylabel('Uncertainty (std)', fontsize=12)\n",
    "axes[0].set_title('Uncertainty Decomposition', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Scatter plot\n",
    "axes[1].scatter(epistemic_std, aleatoric_std, alpha=0.6, s=80, edgecolors='k', linewidth=1)\n",
    "axes[1].set_xlabel('Epistemic Uncertainty', fontsize=12)\n",
    "axes[1].set_ylabel('Aleatoric Uncertainty', fontsize=12)\n",
    "axes[1].set_title('Epistemic vs Aleatoric', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Analysis\n",
    "\n",
    "Check if our uncertainty estimates are well-calibrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute calibration\n",
    "calibration = compute_calibration(y_test, mean_pred, std_pred, n_bins=10)\n",
    "\n",
    "print(f\"Mean Absolute Calibration Error: {calibration['mace']:.4f}\")\n",
    "print(\"(Lower is better, 0 = perfect calibration)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot calibration curve\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "conf = calibration['confidence_levels']\n",
    "obs = calibration['observed_coverage']\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect Calibration')\n",
    "plt.plot(conf, obs, 'o-', linewidth=3, markersize=10, color='#2E86AB', label='Model')\n",
    "\n",
    "plt.xlabel('Expected Coverage', fontsize=13)\n",
    "plt.ylabel('Observed Coverage', fontsize=13)\n",
    "plt.title(f'Calibration Curve\\n(MACE = {calibration[\"mace\"]:.4f})', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3ï¸âƒ£ Confidence Weighting\n",
    "\n",
    "Sometimes data points have different reliability. We can weight them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data with Varying Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with heteroscedastic noise\n",
    "n_train = 150\n",
    "X_noisy = np.random.randn(n_train, 50)\n",
    "\n",
    "# True function\n",
    "y_true = X_noisy[:, 0] + 2*X_noisy[:, 1] + np.sin(X_noisy[:, 2])\n",
    "\n",
    "# Noise level varies with input\n",
    "noise_level = 0.1 + 0.5 * np.abs(X_noisy[:, 0])  # Higher noise for larger |x[0]|\n",
    "y_noisy = y_true + noise_level * np.random.randn(n_train)\n",
    "\n",
    "print(f\"Noise level range: [{noise_level.min():.3f}, {noise_level.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize noise distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_noisy[:, 0], y_noisy, c=noise_level, s=50, cmap='YlOrRd', edgecolors='k', linewidth=0.5)\n",
    "plt.colorbar(label='Noise Level')\n",
    "plt.xlabel('X[0]', fontsize=12)\n",
    "plt.ylabel('Y (with noise)', fontsize=12)\n",
    "plt.title('Data with Varying Noise', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(noise_level, bins=20, edgecolor='k', alpha=0.7, color='#A23B72')\n",
    "plt.xlabel('Noise Level', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Noise Distribution', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with Confidence Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confidence weights (inverse of noise level)\n",
    "# Lower noise -> Higher confidence\n",
    "confidence = 1.0 / (1.0 + noise_level**2)\n",
    "\n",
    "print(f\"Confidence range: [{confidence.min():.3f}, {confidence.max():.3f}]\")\n",
    "print(f\"High noise -> Low confidence: {confidence[noise_level > 0.4].mean():.3f}\")\n",
    "print(f\"Low noise -> High confidence: {confidence[noise_level < 0.2].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train WITHOUT confidence weights\n",
    "print(\"Training WITHOUT confidence weighting...\")\n",
    "_, _, model_no_weight, _ = fit_dkgp(\n",
    "    X_noisy, y_noisy,\n",
    "    feature_dim=16,\n",
    "    num_epochs=800,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train WITH confidence weights\n",
    "print(\"Training WITH confidence weighting...\")\n",
    "_, _, model_weighted, _ = fit_dkgp(\n",
    "    X_noisy, y_noisy,\n",
    "    confidence_weights=confidence,\n",
    "    feature_dim=16,\n",
    "    num_epochs=800,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"âœ… Both models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate clean test data\n",
    "X_test_clean = np.random.randn(50, 50)\n",
    "y_test_clean = X_test_clean[:, 0] + 2*X_test_clean[:, 1] + np.sin(X_test_clean[:, 2])\n",
    "\n",
    "# Predictions\n",
    "pred_no_weight, _ = predict(model_no_weight, X_test_clean)\n",
    "pred_weighted, _ = predict(model_weighted, X_test_clean)\n",
    "\n",
    "# Compute errors\n",
    "mse_no_weight = np.mean((y_test_clean - pred_no_weight)**2)\n",
    "mse_weighted = np.mean((y_test_clean - pred_weighted)**2)\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"  Without weighting: MSE = {mse_no_weight:.4f}\")\n",
    "print(f\"  With weighting:    MSE = {mse_weighted:.4f}\")\n",
    "print(f\"  Improvement:       {(1 - mse_weighted/mse_no_weight)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test_clean, pred_no_weight, alpha=0.6, s=80, label='Without Weighting')\n",
    "plt.scatter(y_test_clean, pred_weighted, alpha=0.6, s=80, label='With Weighting')\n",
    "min_val = y_test_clean.min()\n",
    "max_val = y_test_clean.max()\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect')\n",
    "plt.xlabel('True Values', fontsize=12)\n",
    "plt.ylabel('Predicted Values', fontsize=12)\n",
    "plt.title('Predictions: Weighted vs Unweighted', fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "errors_no_weight = np.abs(y_test_clean - pred_no_weight)\n",
    "errors_weighted = np.abs(y_test_clean - pred_weighted)\n",
    "x = np.arange(len(y_test_clean))\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, errors_no_weight, width, label='Without Weighting', alpha=0.8)\n",
    "plt.bar(x + width/2, errors_weighted, width, label='With Weighting', alpha=0.8)\n",
    "plt.xlabel('Test Sample', fontsize=12)\n",
    "plt.ylabel('Absolute Error', fontsize=12)\n",
    "plt.title('Error Comparison', fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4ï¸âƒ£ Bayesian Optimization\n",
    "\n",
    "Use Deep Kernel GP to optimize an expensive black-box function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def branin_high_dim(x):\n",
    "    \"\"\"\n",
    "    Branin function (classic optimization benchmark) in high dimensions.\n",
    "    Only first 2 dimensions matter, rest are noise.\n",
    "    Global minimum â‰ˆ 0.398 at (Ï€, 2.275) and two other locations.\n",
    "    \"\"\"\n",
    "    x1 = x[0] * 15 - 5   # Scale to [-5, 10]\n",
    "    x2 = x[1] * 15        # Scale to [0, 15]\n",
    "    \n",
    "    a = 1\n",
    "    b = 5.1 / (4 * np.pi**2)\n",
    "    c = 5 / np.pi\n",
    "    r = 6\n",
    "    s = 10\n",
    "    t = 1 / (8 * np.pi)\n",
    "    \n",
    "    term1 = a * (x2 - b * x1**2 + c * x1 - r)**2\n",
    "    term2 = s * (1 - t) * np.cos(x1)\n",
    "    term3 = s\n",
    "    \n",
    "    # Add small noise from other dimensions\n",
    "    if len(x) > 2:\n",
    "        noise = 0.05 * np.sum(x[2:]**2)\n",
    "    else:\n",
    "        noise = 0\n",
    "    \n",
    "    return term1 + term2 + term3 + noise\n",
    "\n",
    "print(f\"True global minimum: â‰ˆ 0.398\")\n",
    "print(f\"Test evaluation: {branin_high_dim(np.array([0.5424, 0.1517] + [0]*18)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize with Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "input_dim = 20\n",
    "n_initial = 10\n",
    "n_iterations = 15\n",
    "\n",
    "# Generate candidate pool\n",
    "n_candidates = 2000\n",
    "candidates = np.random.uniform(0, 1, size=(n_candidates, input_dim))\n",
    "\n",
    "# Initial random samples\n",
    "initial_idx = np.random.choice(n_candidates, n_initial, replace=False)\n",
    "X_observed = candidates[initial_idx]\n",
    "y_observed = np.array([branin_high_dim(x) for x in X_observed])\n",
    "\n",
    "print(f\"Initial samples: {n_initial}\")\n",
    "print(f\"Initial best: {y_observed.min():.4f}\")\n",
    "print(f\"Candidate pool: {n_candidates} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Bayesian Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track progress\n",
    "best_values = [y_observed.min()]\n",
    "all_values = list(y_observed)\n",
    "ei_values_history = []\n",
    "\n",
    "print(\"Starting Bayesian Optimization...\\n\")\n",
    "print(f\"{'Iter':<6} {'Next f(x)':<12} {'Best f(x)':<12} {'Max EI':<12}\")\n",
    "print(\"=\"*48)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    # Train model on current observations\n",
    "    _, _, model, _ = fit_dkgp(\n",
    "        X_observed,\n",
    "        y_observed,\n",
    "        feature_dim=16,\n",
    "        num_epochs=500,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Get current best\n",
    "    best_f = y_observed.min()\n",
    "    \n",
    "    # Remove already evaluated candidates\n",
    "    mask = np.ones(len(candidates), dtype=bool)\n",
    "    for x in X_observed:\n",
    "        mask &= ~np.all(np.isclose(candidates, x), axis=1)\n",
    "    available = candidates[mask]\n",
    "    \n",
    "    # Compute Expected Improvement\n",
    "    ei = expected_improvement(\n",
    "        model,\n",
    "        available,\n",
    "        best_f=best_f,\n",
    "        xi=0.01,\n",
    "        maximize=False  # Minimize Branin\n",
    "    )\n",
    "    \n",
    "    # Select next point\n",
    "    next_idx = np.argmax(ei)\n",
    "    next_point = available[next_idx]\n",
    "    next_value = branin_high_dim(next_point)\n",
    "    \n",
    "    # Update observations\n",
    "    X_observed = np.vstack([X_observed, next_point])\n",
    "    y_observed = np.append(y_observed, next_value)\n",
    "    \n",
    "    # Track progress\n",
    "    best_values.append(y_observed.min())\n",
    "    all_values.append(next_value)\n",
    "    ei_values_history.append(ei.max())\n",
    "    \n",
    "    print(f\"{iteration+1:<6} {next_value:<12.4f} {y_observed.min():<12.4f} {ei.max():<12.6f}\")\n",
    "\n",
    "print(\"=\"*48)\n",
    "print(f\"\\nOptimization complete!\")\n",
    "print(f\"Final best value: {y_observed.min():.4f}\")\n",
    "print(f\"True global minimum: 0.398\")\n",
    "print(f\"Gap: {y_observed.min() - 0.398:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Optimization Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Best value over iterations\n",
    "axes[0, 0].plot(best_values, 'o-', linewidth=2, markersize=8, color='#2E86AB')\n",
    "axes[0, 0].axhline(y=0.398, color='r', linestyle='--', linewidth=2, label='Global minimum')\n",
    "axes[0, 0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Best f(x) Found', fontsize=12)\n",
    "axes[0, 0].set_title('Optimization Progress', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. All evaluations\n",
    "axes[0, 1].plot(all_values, 'o', alpha=0.6, markersize=6, color='#A23B72')\n",
    "axes[0, 1].axhline(y=0.398, color='r', linestyle='--', linewidth=2, label='Global minimum')\n",
    "axes[0, 1].axvline(x=n_initial-0.5, color='k', linestyle=':', linewidth=2, label='Initial samples')\n",
    "axes[0, 1].set_xlabel('Evaluation', fontsize=12)\n",
    "axes[0, 1].set_ylabel('f(x)', fontsize=12)\n",
    "axes[0, 1].set_title('All Evaluations', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Expected Improvement over iterations\n",
    "axes[1, 0].plot(ei_values_history, 's-', linewidth=2, markersize=7, color='#F18F01')\n",
    "axes[1, 0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Max Expected Improvement', fontsize=12)\n",
    "axes[1, 0].set_title('Acquisition Function Values', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Cumulative improvement\n",
    "improvements = np.maximum.accumulate(-np.array(best_values))  # Negative for minimization\n",
    "axes[1, 1].plot(improvements, 'o-', linewidth=2, markersize=8, color='#C73E1D')\n",
    "axes[1, 1].set_xlabel('Iteration', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Cumulative Improvement', fontsize=12)\n",
    "axes[1, 1].set_title('Total Improvement Over Initial Best', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Acquisition Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model\n",
    "_, _, final_model, _ = fit_dkgp(\n",
    "    X_observed, y_observed,\n",
    "    feature_dim=16,\n",
    "    num_epochs=500,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Sample some test candidates\n",
    "test_candidates = np.random.uniform(0, 1, (100, input_dim))\n",
    "best_f = y_observed.min()\n",
    "\n",
    "# Compute different acquisition functions\n",
    "ei = expected_improvement(final_model, test_candidates, best_f, maximize=False)\n",
    "ucb = upper_confidence_bound(final_model, test_candidates, beta=2.0, maximize=False)\n",
    "pi = probability_of_improvement(final_model, test_candidates, best_f, maximize=False)\n",
    "\n",
    "# Normalize for comparison\n",
    "ei_norm = (ei - ei.min()) / (ei.max() - ei.min() + 1e-8)\n",
    "ucb_norm = (ucb - ucb.min()) / (ucb.max() - ucb.min() + 1e-8)\n",
    "pi_norm = (pi - pi.min()) / (pi.max() - pi.min() + 1e-8)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "x = np.arange(len(test_candidates))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x - width, ei_norm, width, label='EI', alpha=0.8)\n",
    "plt.bar(x, ucb_norm, width, label='UCB', alpha=0.8)\n",
    "plt.bar(x + width, pi_norm, width, label='PI', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Candidate Index', fontsize=12)\n",
    "plt.ylabel('Normalized Acquisition Value', fontsize=12)\n",
    "plt.title('Comparison of Acquisition Functions', fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show top candidates for each\n",
    "print(\"\\nTop 3 candidates by acquisition function:\")\n",
    "print(f\"  EI:  indices {np.argsort(ei)[-3:][::-1]}\")\n",
    "print(f\"  UCB: indices {np.argsort(ucb)[:3]}\")\n",
    "print(f\"  PI:  indices {np.argsort(pi)[-3:][::-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“š Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. âœ… **Basic Regression**: Training on high-dimensional data\n",
    "2. âœ… **Uncertainty Quantification**: Epistemic vs aleatoric uncertainty\n",
    "3. âœ… **Confidence Weighting**: Handling varying data quality\n",
    "4. âœ… **Bayesian Optimization**: Using acquisition functions\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Deep Kernel GP combines neural networks with GPs for high-dimensional regression\n",
    "- Provides well-calibrated uncertainty estimates\n",
    "- Confidence weighting improves performance on noisy data\n",
    "- Multiple acquisition functions available for Bayesian optimization\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try on your own dataset\n",
    "- Experiment with different architectures (`feature_dim`, `hidden_dims`)\n",
    "- Compare acquisition functions on your optimization problem\n",
    "- Save/load models with `utils.save_model()` and `utils.load_model()`\n",
    "\n",
    "Happy modeling! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DKGP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
